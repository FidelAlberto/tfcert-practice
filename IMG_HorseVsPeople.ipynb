{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "IMG_HorseVsPeople.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPDs83ACFENMNuzep6IwvX8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/btcnhung1299/tf-practice/blob/master/IMG_HorseVsPeople.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qliP-yurmxWC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install -q Pillow"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IF9qPZiCmvGr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import tensorflow as tf"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "do63x1qZmqd_",
        "colab_type": "text"
      },
      "source": [
        "## Data Gathering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZvXvBS1VnATe",
        "colab_type": "text"
      },
      "source": [
        "Download dataset and extract to `train_data` and `val_data`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dJt89tXsfI0E",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 386
        },
        "outputId": "9397ec97-8a42-4678-81af-ffc54731376c"
      },
      "source": [
        "!wget http://storage.googleapis.com/laurencemoroney-blog.appspot.com/horse-or-human.zip\n",
        "!wget http://storage.googleapis.com/laurencemoroney-blog.appspot.com/validation-horse-or-human.zip"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-09-10 16:07:49--  http://storage.googleapis.com/laurencemoroney-blog.appspot.com/horse-or-human.zip\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 74.125.28.128, 74.125.142.128, 74.125.195.128, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|74.125.28.128|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 149574867 (143M) [application/zip]\n",
            "Saving to: ‘horse-or-human.zip’\n",
            "\n",
            "horse-or-human.zip  100%[===================>] 142.65M   117MB/s    in 1.2s    \n",
            "\n",
            "2020-09-10 16:07:51 (117 MB/s) - ‘horse-or-human.zip’ saved [149574867/149574867]\n",
            "\n",
            "--2020-09-10 16:07:51--  http://storage.googleapis.com/laurencemoroney-blog.appspot.com/validation-horse-or-human.zip\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 74.125.142.128, 74.125.195.128, 173.194.202.128, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|74.125.142.128|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 11480187 (11M) [application/zip]\n",
            "Saving to: ‘validation-horse-or-human.zip’\n",
            "\n",
            "validation-horse-or 100%[===================>]  10.95M  59.6MB/s    in 0.2s    \n",
            "\n",
            "2020-09-10 16:07:51 (59.6 MB/s) - ‘validation-horse-or-human.zip’ saved [11480187/11480187]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rMZnLP39fbzU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "TRAIN_DIR = \"./train_data\"\n",
        "VAL_DIR = \"./val_data\"\n",
        "!unzip -q horse-or-human.zip -d $TRAIN_DIR\n",
        "!unzip -q validation-horse-or-human.zip -d $VAL_DIR"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PJ2wRkgfnP1n",
        "colab_type": "text"
      },
      "source": [
        "Read a sample image and check pixel range."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X7DYtpNXldJB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_sample_fn = os.listdir(os.path.join(TRAIN_DIR, \"horses\"))[0]\n",
        "train_sample_path = os.path.join(TRAIN_DIR, \"horses\", train_sample_fn)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jD-D_FlhuQaz",
        "colab_type": "text"
      },
      "source": [
        "**Note:** The last channel is transparency."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6s_waGxWi-zk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 168
        },
        "outputId": "e0622f0c-e5d3-453e-f059-88f8f7e098f1"
      },
      "source": [
        "img = Image.open(train_sample_path)\n",
        "img_data = np.asarray(img)\n",
        "\n",
        "print(\"Image shape:\", img_data.shape)\n",
        "print(\"Pixel in range:\", np.min(img_data), np.max(img_data))\n",
        "\n",
        "print(img_data[:1])"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Image shape: (300, 300, 4)\n",
            "Pixel in range: 1 255\n",
            "[[[ 34  28  27 255]\n",
            "  [ 35  29  28 255]\n",
            "  [ 34  28  27 255]\n",
            "  ...\n",
            "  [ 31 110 173 255]\n",
            "  [ 31 109 172 255]\n",
            "  [ 34 109 170 255]]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2_rqpGVVoQPw",
        "colab_type": "text"
      },
      "source": [
        "## Data Flow"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B--ZxrkMqZjL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "img_size = (150, 150)\n",
        "img_shape = (*img_size, 3)    # We didn't include transparency channel"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T-d1RouhobBt",
        "colab_type": "text"
      },
      "source": [
        "Create data flow from a directory, each image is scaled to the range 0-255."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eoHgnUAckbGV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "def gen_new_data(data_folder):\n",
        "  data_gen = ImageDataGenerator(rescale=1./255)\n",
        "  data = data_gen.flow_from_directory(\n",
        "      data_folder,\n",
        "      target_size=img_size,\n",
        "      batch_size=64,\n",
        "      class_mode=\"binary\",\n",
        "  )\n",
        "  return data"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QoRQk6v1hYck",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "b28dfbec-3b36-48e0-8e8f-8fe5d17cb28f"
      },
      "source": [
        "train_data_gen = gen_new_data(TRAIN_DIR)\n",
        "val_data_gen = gen_new_data(VAL_DIR)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 1027 images belonging to 2 classes.\n",
            "Found 256 images belonging to 2 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "79TfCpsMdptV",
        "colab_type": "text"
      },
      "source": [
        "## Load Pretrained Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZAkkPfEkr5pO",
        "colab_type": "text"
      },
      "source": [
        "Load InceptionV3 pretrained on ImageNet and freeze all layers above `mixed7`. This layer will give us features extracted from InceptionV3. We're gonna replace the top of the model with our own classifier."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uXUSfTndd_QP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "c784bf25-11ba-4b1b-a33c-9922ae133feb"
      },
      "source": [
        "from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
        "pretrained_model = InceptionV3(include_top=False, weights=\"imagenet\", input_shape=(150, 150, 3))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/inception_v3/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "87916544/87910968 [==============================] - 1s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qN9Wu2XZoXMb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8517e325-12cf-4c7b-b64e-1222fc3ea1e9"
      },
      "source": [
        "# Freeze all layers\n",
        "for layer in pretrained_model.layers:\n",
        "  layer.trainable = False\n",
        "\n",
        "# We keep all layers up to `mixed7`\n",
        "last_pretrained_layer = pretrained_model.get_layer(name=\"mixed7\")\n",
        "print(\"Last layer shape:\", last_pretrained_layer.output_shape)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Last layer shape: (None, 7, 7, 768)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mOCR_2sOo0jZ",
        "colab_type": "text"
      },
      "source": [
        "## Model Architecture & Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "40yHxP5Zoswt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPooling2D, Dropout\n",
        "from tensorflow.keras.layers.experimental.preprocessing import Rescaling"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bhQHh_J7ixu_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_model = Flatten()(last_pretrained_layer.output)\n",
        "train_model = Dense(1024, activation=\"swish\")(train_model)\n",
        "train_model = Dropout(0.1)(train_model)\n",
        "train_model = Dense(1, activation=\"sigmoid\")(train_model)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pgJTRlZemSN5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = Model(pretrained_model.input, train_model)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0E8deF5imo5A",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        },
        "outputId": "8620eb5d-b65d-4ca5-ce48-ebd3f0fb9918"
      },
      "source": [
        "optimizer = tf.keras.optimizers.RMSprop(learning_rate=1e-4)\n",
        "model.compile(optimizer=\"rmsprop\", loss=\"binary_crossentropy\", metrics=[\"acc\"])\n",
        "model.fit(train_data_gen, epochs=5, validation_data=val_data_gen)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "17/17 [==============================] - 8s 442ms/step - loss: 0.8365 - acc: 0.9562 - val_loss: 0.0552 - val_acc: 0.9922\n",
            "Epoch 2/5\n",
            "17/17 [==============================] - 6s 380ms/step - loss: 0.0063 - acc: 0.9981 - val_loss: 0.0021 - val_acc: 1.0000\n",
            "Epoch 3/5\n",
            "17/17 [==============================] - 6s 375ms/step - loss: 3.8852e-08 - acc: 1.0000 - val_loss: 0.0021 - val_acc: 1.0000\n",
            "Epoch 4/5\n",
            "17/17 [==============================] - 6s 375ms/step - loss: 2.0600e-09 - acc: 1.0000 - val_loss: 0.0021 - val_acc: 1.0000\n",
            "Epoch 5/5\n",
            "17/17 [==============================] - 6s 374ms/step - loss: 1.6268e-09 - acc: 1.0000 - val_loss: 0.0021 - val_acc: 1.0000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f4aac1358d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    }
  ]
}